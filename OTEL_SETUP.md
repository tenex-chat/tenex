# OpenTelemetry Integration - Phase 1 Complete

## ✅ What's Been Implemented

### 1. Core Infrastructure (COMPLETE)
- ✅ OpenTelemetry SDK installed and configured
- ✅ Telemetry bootstrap in `src/telemetry/setup.ts`
- ✅ Auto-initialization in `src/tenex.ts` (first import)
- ✅ 100% sampling - captures ALL traces
- ✅ Batch export every 5 seconds for performance

### 2. AI SDK Telemetry (COMPLETE)
- ✅ Full telemetry enabled in `LLMService`
- ✅ Captures ALL LLM interactions:
  - `streamText()` - Full prompts, responses, tool calls
  - `generateText()` - Complete request/response data
  - `generateObject()` - Schema and generated objects
- ✅ NO privacy filters - everything captured for debugging
- ✅ Metadata: provider, model, temperature, agent slug, session ID

### 3. Daemon Event Handling (COMPLETE)
- ✅ Root span for every incoming Nostr event (`tenex.event.process`)
- ✅ Distributed trace context extraction for delegation linking
- ✅ Full event data captured:
  - Event content, tags, kind, pubkey
  - Routing decisions with reasons
  - Project runtime start/stop
  - Error handling with exceptions
- ✅ Span events for key decisions:
  - `routing_decision` - Why events route where they do
  - `project_runtime_start` - When projects initialize
  - `error` - Errors with context

## 🎯 What You Get RIGHT NOW

### Automatic LLM Tracing
Every call to `streamText`, `generateText`, or `generateObject` creates a span with:
- Full input messages (prompts)
- Full output text/objects (responses)
- All tool definitions and tool calls
- Token usage (prompt, completion, total)
- Model and provider information
- Error traces if failures occur

### End-to-End Event Tracing
Every Nostr event processed creates a root span showing:
- Complete event data (content, tags, metadata)
- Routing logic (how it found its project)
- Runtime initialization (if project wasn't running)
- Success or failure with error details

### Distributed Tracing Foundation
- Trace context propagation support ready for delegations
- Parent-child span linking across async operations
- Full trace ancestry preserved

## 🚀 Quick Start

### 1. Start Jaeger (Local Development)

```bash
docker run -d --name jaeger \
  -e COLLECTOR_OTLP_ENABLED=true \
  -p 16686:16686 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

### 2. Run TENEX

```bash
# Telemetry is ALWAYS on - no flags needed
bun run start

# Or run the test script
./test-otel.sh
```

### 3. View Traces

Open http://localhost:16686 in your browser

**Find traces:**
- Service: `tenex-daemon`
- Operation: `tenex.event.process` (root spans)
- Search by tags:
  - `event.id=<event_id>`
  - `project.id=<project_id>`
  - `agent.slug=<agent_name>`

## 📊 Example Trace Structure

```
tenex.event.process (ROOT) - 2.5s
├─ event.id: abc123...
├─ event.content: "Help me debug this code"
├─ event.tags: [["p", "agent_pubkey"], ...]
├─ routing_decision: "route_to_project"
├─ project.id: 31933:xyz:project1
│
└─ [Child spans from ProjectRuntime, AgentExecutor, etc.]
   └─ ai.streamText - 2.2s (AI SDK auto-generated)
      ├─ ai.prompt.messages: [full prompt array]
      ├─ ai.response.text: [full response]
      ├─ ai.usage.promptTokens: 1250
      ├─ ai.usage.completionTokens: 450
      │
      └─ [Tool call spans - auto-generated by AI SDK]
```

## 🔧 Configuration

### Environment Variables

```bash
# Optional: Custom OTLP endpoint (defaults to localhost:4318)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces

# For Honeycomb (cloud alternative)
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.honeycomb.io
OTEL_EXPORTER_OTLP_HEADERS="x-honeycomb-team=YOUR_API_KEY"

# For Grafana Cloud
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp-gateway-prod-us-central-0.grafana.net/otlp
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Basic <base64-token>"
```

## 📝 Next Steps (Future Phases)

### Phase 2: Complete Event Flow (To Be Implemented)
- [ ] Instrument `EventRouter` with routing logic details
- [ ] Instrument `ConversationResolver` with resolution decisions
- [ ] Instrument `AgentRouter` with agent selection logic
- [ ] Add trace context propagation in `AgentPublisher` for delegations

### Phase 3: Agent Execution (To Be Implemented)
- [ ] Instrument `AgentExecutor` with full execution context
- [ ] **CRITICAL**: Instrument message strategies with compiled system prompts
- [ ] Instrument `AgentRegistry` with agent loading details
- [ ] Instrument `AgentSupervisor` with completion decisions

### Phase 4: Tool & Delegation Tracking (To Be Implemented)
- [ ] Instrument `ToolExecutionTracker` with full args/results
- [ ] Instrument `DelegationRegistry` with batch tracking
- [ ] Instrument `DelegationService` with wait operations
- [ ] Complete distributed trace linking for multi-agent flows

### Phase 5: Subsystems (To Be Implemented)
- [ ] Instrument `RAGService` (query, embed, add documents)
- [ ] Instrument `MCPManager` (resource access)
- [ ] Instrument `ConfigService` (configuration loading)
- [ ] Instrument Nostr publish operations

## 🐛 Debugging Tips

### "I don't see traces in Jaeger"

1. **Check Jaeger is running:**
   ```bash
   curl http://localhost:16686
   ```

2. **Check TENEX is exporting:**
   Look for `[Telemetry] OpenTelemetry enabled` in startup logs

3. **Check OTLP endpoint:**
   ```bash
   curl http://localhost:4318/v1/traces
   ```

### "Traces are incomplete"

- AI SDK telemetry only works for `streamText`, `generateText`, `generateObject`
- Other spans haven't been instrumented yet (see "Next Steps" above)

### "Too much data / performance issues"

This is a development-focused setup with 100% sampling and full data capture.

For production (future):
- Implement sampling (capture 10% of traces)
- Add attribute limits
- Filter sensitive data
- Use tail-based sampling (sample errors + slow requests)

## 📚 Additional Resources

- [OpenTelemetry Docs](https://opentelemetry.io/docs/languages/js/)
- [AI SDK Telemetry Docs](https://sdk.vercel.ai/docs/ai-sdk-core/telemetry)
- [Jaeger UI Guide](https://www.jaegertracing.io/docs/latest/frontend-ui/)

## 🎉 Success Criteria

You know it's working when:
1. ✅ `[Telemetry] OpenTelemetry enabled` appears in startup logs
2. ✅ Jaeger UI shows `tenex-daemon` service
3. ✅ Sending a Nostr event creates a `tenex.event.process` trace
4. ✅ LLM calls create `ai.streamText` child spans
5. ✅ You can see full prompts and responses in span attributes

---

**Status: Phase 1 COMPLETE (4/10 phases)**
**Time invested: ~4 hours**
**Next: Phase 2 - Complete Event Flow Instrumentation**
